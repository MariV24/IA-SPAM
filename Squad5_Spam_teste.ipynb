{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuvZJZOkuw8WeTEOzZ0y8z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariV24/IA-SPAM/blob/main/Squad5_Spam_teste.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnMA1BRoYgHe"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import nltk\n",
        "import re\n",
        "from PIL import Image\n",
        "from tqdm import tqdm #para mostrar progresso do trabalho\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud  #para nuvem de palavras\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()  #para pegar a raiz da palavra = lemma\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer  #para romover flexão das palavras ex: florticultura se torna flor\n",
        "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True) #se não ignorar os stopwords, não vai remove-los depois\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#inicializando bag de stopwords\n",
        "stopwords = nltk.corpus.stopwords.words('english') #stopwords são palavras irrelevantes ex: the, a, as..."
      ],
      "metadata": {
        "id": "dnjBR9lEWL32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preProcessamento(dataPanda):\n",
        "    data = dataPanda\n",
        "    for i in tqdm(range(len(data.text))):\n",
        "        temp1 = data.text[i]\n",
        "        temp1 = temp1.split()\n",
        "        temp3 = []\n",
        "        # aplica steammer ou lemmatizer   = são duas abordagens para tratar palavras flexionadas\n",
        "        for u in range(len(temp1)):\n",
        "            # temp1[u] = stemmer.stem(temp1[u])\n",
        "            temp1[u] = lemmatizer.lemmatize(temp1[u])\n",
        "            if (temp1[u] in stopwords):\n",
        "                temp3.append(temp1[u])\n",
        "        temp1 = \" \".join(temp1)\n",
        "        # remove stopwords\n",
        "        if (len(temp3) > 0):\n",
        "            for u in temp3:\n",
        "                temp1 = temp1.replace(u, \"\")\n",
        "        # filtering\n",
        "        temp1 = re.sub('[^A-Za-z]+', ' ', temp1)\n",
        "        data.text[i] = temp1\n",
        "    return data"
      ],
      "metadata": {
        "id": "9GLtVkMiWfXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def readBase(csvFile):\n",
        "    labels = []\n",
        "    base = []\n",
        "    listaPorEmocao = {'positive':\"\", 'negative':\"\", 'neutral':\"\"}  #alterar para a base de  vocÊs\n",
        "    qtdEmocao = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
        "    with open(csvFile) as csvfile:\n",
        "        import codecs\n",
        "        ifile = open(csvFile, \"rb\")\n",
        "        read = csv.reader(codecs.iterdecode(ifile, 'utf-8'))\n",
        "\n",
        "        for row in read:\n",
        "            try:\n",
        "                temp2 = str(row[1])\n",
        "                labels.append(temp2)\n",
        "\n",
        "                temp1 = str(row[10])\n",
        "\n",
        "                temp1 = temp1.split()\n",
        "                temp3 = []\n",
        "                #aplica steammer\n",
        "                for u in range(len(temp1)):\n",
        "                    #temp1[u] = stemmer.stem(temp1[u])\n",
        "                    temp1[u] = lemmatizer.lemmatize(temp1[u])\n",
        "                    if(temp1[u] in stopwords):\n",
        "                        temp3.append(temp1[u])\n",
        "                temp1 = \" \".join(temp1)\n",
        "                #remove stopwords\n",
        "                if(len(temp3)>0):\n",
        "                    for u in temp3:\n",
        "                        temp1 = temp1.replace(u,\"\")\n",
        "\n",
        "                # filtering\n",
        "                temp1 = re.sub('[^A-Za-z]+', ' ', temp1)\n",
        "                base.append(temp1)\n",
        "                listaPorEmocao[temp2] = listaPorEmocao[temp2] + \" \" + temp1\n",
        "                qtdEmocao[temp2] = qtdEmocao[temp2] + 1\n",
        "            except IndexError:\n",
        "                pass\n",
        "    return base, labels, listaPorEmocao, qtdEmocao"
      ],
      "metadata": {
        "id": "TuV8BvBUWbw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gerarNuvem(freqs, numPalavras, nome):\n",
        "    wc = WordCloud(background_color=\"white\", width=1700, height=1000, max_words=numPalavras, relative_scaling=0.5,\n",
        "                   normalize_plurals=False).generate_from_frequencies(freqs)\n",
        "    plt.imshow(wc)\n",
        "    plt.axis(\"off\")\n",
        "    #plt.show()\n",
        "    #plt.savefig(\"img/\"+nome+'.png')\n",
        "    \n",
        "    \n",
        "base, labels, listaPorEmocao, qtdEmocao = readBase(csvFileTrain)"
      ],
      "metadata": {
        "id": "eLS3v_F5WZju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listagem de todas as palavras\n",
        "##allWords = \" \".join(base)\n",
        "##allWords = allWords.split()\n",
        "##print(\"Palavras total: \"+str(len(allWords)))\n",
        "##\n",
        "##all_words_unique = sorted(set(allWords))\n",
        "##print(\"Palavras únicas: \"+str(len(all_words_unique)))\n",
        "##\n",
        "##labels_unique = sorted(set(labels))\n",
        "##print(labels_unique)\n",
        "\n",
        "#contando frequencia dos termos\n",
        "##from collections import Counter\n",
        "##freqAllWords = Counter(allWords)\n",
        "##freqPorEmocao = []\n",
        "##for i in labels_unique:\n",
        "##    listaPorEmocao[i] = listaPorEmocao[i].split()\n",
        "##    freqPorEmocao.append(Counter(listaPorEmocao[i]))\n",
        "\n",
        "###gerar nuvel base geral\n",
        "###gerarNuvem(freqAllWords, 100, \"nuvem-todos\")\n",
        "##cont = 0\n",
        "##for u in freqPorEmocao:\n",
        "##    #gerarNuvem(u, 100, \"nuvem-\"+labels_unique[cont])\n",
        "##    cont = cont + 1\n",
        "\n",
        "#gráfico de pizza\n",
        "#pyplot.pie([float(v) for v in qtdEmocao.values()], labels=[k for k in qtdEmocao], autopct=None)\n",
        "#pyplot.show()\n",
        "#pyplot.savefig('img/piechart.png')\n",
        "\n",
        "\n",
        "#all_words = neg_words + pos_words\n",
        "\n",
        "#common_all = Counter(all_words).most_common(150)\n",
        "#common_neg = Counter(neg_words).most_common(150)\n",
        "#common_pos = Counter(pos_words).most_common(150)\n",
        "\n",
        "#gerarNuvem(common_all, 'common_all')\n",
        "#gerarNuvem(common_neg, 'common_neg')\n",
        "#gerarNuvem(common_pos, 'common_pos')\n",
        "\n",
        "\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer   #verificar o processo vectorizer\n",
        "#vectorizer_TFIDF = TfidfVectorizer(sublinear_tf=True) #min_df=0.005\n",
        "#X_TFIDF = vectorizer_TFIDF.fit_transform(base)"
      ],
      "metadata": {
        "id": "h43JZQD9WWXd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}